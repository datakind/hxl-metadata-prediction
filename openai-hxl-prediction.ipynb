{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8r1UPQinjKj"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "A data standard on platforms such as the [Humanitarian Data Exchange (HDX)](https://data.humdata.org/) is the [Humanitarian Exchange Language (HXL)](https://hxlstandard.org/), a column level set of attributes and tags and attributes which improve data interoperability and discovery. These tags and attributes are typically set by hand by data owners, which being a manual process can result in poor dataset coverage. Improving coverage through ML and AI techniques is desirable for faster and more efficient use of data in responding to Humanitarian disasters.\n",
        "\n",
        "Previous work has focussed on fine tuning LLMs to complete tags and attrubutes, starting with the study [Predicting Metadata on Humanitarian Datasets with GPT 3](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d). This has yielded promosing results, but is constrained by the quality of training data and the HDX team have confirmed that basic tags related to location and dates are popular, more esoteric tags defined in [the standard](https://hxlstandard.org/standard/1-1final/tagging/) are not well represented.\n",
        "\n",
        "This notebook fine-tunes an OpenAI model to test performance.\n",
        "\n",
        "# Setup\n",
        "\n",
        "1. Run notebook [generate-test-train-data.ipynb]([generate-test-train-data.ipynb]) to generate test and train data files for use in fine-tuning\n",
        "2. Set `OPENAI_API_KEY` in file `.env` or in Colab secrets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==2.2.2\n",
        "!pip install openai==1.35.3\n",
        "!pip install python-dotenv==1.0.1"
      ],
      "metadata": {
        "id": "fwGQHZ5foEUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  from google.colab import userdata\n",
        "  OPENAI_API_KEY =  userdata.get('OPENAI_API_KEY')\n",
        "else:\n",
        "  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# If using Colab, this is where Google drive gets mounted. Otherwise leave blank\n",
        "GOOGLE_BASE_DIR = \"/content/drive/MyDrive/Colab\"\n",
        "\n",
        "# Where to save local data files\n",
        "LOCAL_DATA_DIR = f\"{GOOGLE_BASE_DIR}/hxl-metadata-prediction/data/\"\n",
        "\n",
        "# As generated by generate-test-train-data.ipynb\n",
        "TRAINING_FILE = f\"{LOCAL_DATA_DIR}/hxl_chat_prompts.jsonl\""
      ],
      "metadata": {
        "id": "aPrG5T97n8eF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UK6zb6juqwil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "## Fine-tune using Open AI"
      ],
      "metadata": {
        "id": "vrptvCe-p-ES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_mYaiUQE8KHu"
      },
      "outputs": [],
      "source": [
        "def fine_tune_model(train_file, model_name=\"gpt-4o-mini\"):\n",
        "    \"\"\"\n",
        "    Fine-tune an OpenAI model using training data.\n",
        "\n",
        "    Args:\n",
        "        prompt_file (str): The file containing the prompts to use for fine-tuning.\n",
        "        model_name (str): The name of the model to fine-tune. Default is \"davinci-002\".\n",
        "\n",
        "    Returns:\n",
        "        str: The ID of the fine-tuned model.\n",
        "    \"\"\"\n",
        "\n",
        "    client = OpenAI(\n",
        "        api_key=OPENAI_API_KEY\n",
        "    )\n",
        "\n",
        "    # Create a file on OpenAI for fine-tuning\n",
        "    file = client.files.create(\n",
        "        file=open(train_file, \"rb\"),\n",
        "        purpose=\"fine-tune\"\n",
        "    )\n",
        "    file_id = file.id\n",
        "    print(f\"Uploaded training file with ID: {file_id}\")\n",
        "\n",
        "    # Start the fine-tuning job\n",
        "    ft = client.fine_tuning.jobs.create(\n",
        "        training_file=file_id,\n",
        "        model=model_name\n",
        "    )\n",
        "    ft_id = ft.id\n",
        "    print(f\"Fine-tuning job started with ID: {ft_id}\")\n",
        "\n",
        "    # Monitor the status of the fine-tuning job\n",
        "    ft_result = client.fine_tuning.jobs.retrieve(ft_id)\n",
        "    while ft_result.status != 'succeeded':\n",
        "        print(f\"Current status: {ft_result.status}\")\n",
        "        time.sleep(120)  # Wait for 60 seconds before checking again\n",
        "        ft_result = client.fine_tuning.jobs.retrieve(ft_id)\n",
        "        if 'failed' in ft_result.status.lower():\n",
        "            sys.exit()\n",
        "\n",
        "    print(f\"Fine-tuning job {ft_id} succeeded!\")\n",
        "\n",
        "    # Retrieve the fine-tuned model\n",
        "    fine_tuned_model = ft_result.fine_tuned_model\n",
        "    print(f\"Fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "    return fine_tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MIvUHlXfnjKl",
        "outputId": "4d3cfd98-9153-410c-acd6-50cd0cbb7238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded training file with ID: file-CEZdeKIIj7PoQ7L8FePyWtvI\n",
            "Fine-tuning job started with ID: ftjob-ULboB88nzvwFO25bqgIF6QvB\n",
            "Current status: validating_files\n",
            "Current status: validating_files\n",
            "Current status: queued\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-995213785055>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini-2024-07-18-alpha\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-81a5dada5884>\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(train_file, model_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mft_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'succeeded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current status: {ft_result.status}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for 60 seconds before checking again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mft_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tuning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'failed'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mft_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "fine_tune_model(TRAINING_FILE, model_name=\"gpt-4o-mini-2024-07-18-alpha\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNBF0uBIqDUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4ueIhX8qpOE",
        "outputId": "66785a83-3959-482a-ae04-e80cf00ed69a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}