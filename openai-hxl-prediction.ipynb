{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "A data standard on platforms such as the [Humanitarian Data Exchange (HDX)](https://data.humdata.org/) is the [Humanitarian Exchange Language (HXL)](https://hxlstandard.org/), a column level set of attributes and tags and attributes which improve data interoperability and discovery. These tags and attributes are typically set by hand by data owners, which being a manual process can result in poor dataset coverage. Improving coverage through ML and AI techniques is desirable for faster and more efficient use of data in responding to Humanitarian disasters.\n",
        "\n",
        "Previous work has focussed on fine tuning LLMs to complete tags and attrubutes, starting with the study [Predicting Metadata on Humanitarian Datasets with GPT 3](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d). This has yielded promosing results, but is constrained by the quality of training data and the HDX team have confirmed that basic tags related to location and dates are popular, more esoteric tags defined in [the standard](https://hxlstandard.org/standard/1-1final/tagging/) are not well represented.\n",
        "\n",
        "This notebook fine-tunes an OpenAI model to test performance.\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. Create a virtual environment with Python 3.11.4 and run `pip install -r requirements.txt`\n",
        "2. Run notebook [generate-test-train-data.ipynb]([generate-test-train-data.ipynb]) to generate test and train data files for use in fine-tuning\n",
        "3. Set `OPENAI_API_KEY` in file `.env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mYaiUQE8KHu"
      },
      "outputs": [],
      "source": [
        "def fine_tune_model(train_file, model_name=\"gpt-4o-mini\"):\n",
        "    \"\"\"\n",
        "    Fine-tune an OpenAI model using training data.\n",
        "\n",
        "    Args:\n",
        "        prompt_file (str): The file containing the prompts to use for fine-tuning.\n",
        "        model_name (str): The name of the model to fine-tune. Default is \"davinci-002\".\n",
        "\n",
        "    Returns:\n",
        "        str: The ID of the fine-tuned model.\n",
        "    \"\"\"\n",
        "\n",
        "    client = OpenAI(\n",
        "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        "    )\n",
        "\n",
        "    # Create a file on OpenAI for fine-tuning\n",
        "    file = client.files.create(\n",
        "        file=open(train_file, \"rb\"),\n",
        "        purpose=\"fine-tune\"\n",
        "    )\n",
        "    file_id = file.id\n",
        "    print(f\"Uploaded training file with ID: {file_id}\")\n",
        "\n",
        "    # Start the fine-tuning job\n",
        "    ft = client.fine_tuning.jobs.create(\n",
        "        training_file=file_id,\n",
        "        model=model_name\n",
        "    )\n",
        "    ft_id = ft.id\n",
        "    print(f\"Fine-tuning job started with ID: {ft_id}\")\n",
        "\n",
        "    # Monitor the status of the fine-tuning job\n",
        "    ft_result = client.fine_tuning.jobs.retrieve(ft_id)\n",
        "    while ft_result.status != 'succeeded':\n",
        "        print(f\"Current status: {ft_result.status}\")\n",
        "        time.sleep(120)  # Wait for 60 seconds before checking again\n",
        "        ft_result = client.fine_tuning.jobs.retrieve(ft_id)\n",
        "        if 'failed' in ft_result.status.lower():\n",
        "            sys.exit()\n",
        "\n",
        "    print(f\"Fine-tuning job {ft_id} succeeded!\")\n",
        "\n",
        "    # Retrieve the fine-tuned model\n",
        "    fine_tuned_model = ft_result.fine_tuned_model\n",
        "    print(f\"Fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "    return fine_tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fine_tune_model('./data/hxl_chat_prompts.jsonl', model_name=\"gpt-4o-mini-2024-07-18-alpha\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNfF2S8xD4bGP8/6npiRgHM",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
